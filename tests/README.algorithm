Let L(w) be the function that we are trying to find the global minimum of.
Let a 'sample' be a triplet {w, L(w), L'(w)}.

The algorithm has a notion of a 'current' position (w).
The current position is adjusted to a new position, after which a new sample is taken.

In pseudo code:

  Algorithm gda;

  while (gda(w, L(w), dLdw(w)))
    ;

where the Algorithm::operator()(Weight& w, ...) changes w from the 'current' value
upon entry to the new value every time it is called.

When describing the algorithm below, and we say 'step' then this means
the value that is added to the current w to get the new w.

The decision on where to go next is made by looking at
approximations of L based on samples taken.

The main idea of the algorithm is to find a local minimum, and explore
that in a certain direction until a minimum is found that is less deep
than the previous one. The algorithm terminates when it finds a minimum
that has both left and right either no minimum, or minima that are less
deep.

In order to do this, the algorithm searches alternating for maxima and
minima in a given horizontal direction; if a minimum is found that is
less deep than the best minimum found so far, that direction is aborted,
the current position is changed to the position of the best minimum
found so far and then the other side is explored, if that wasn't already
done.

For efficiency and in order to be know when to terminate, extremes are
marked as "being explored to the left/right"; the algorithm terminates
when - after jumping back to the best minimum - that minimum is already
marked as being explored both, to the left and to the right.

An extreme is marked as explored in a given direction if:
* We find the extreme immediately next to it in that direction.
* The extreme is found immediately next to a series of adjacent extremes
  among which at least one minimum. In practise that means: it is either
  a maximum (then we must have come from a minimum), or it is a minimum
  and we came from a maximum that was found after finding a minimum:
  min -> max (current), or
  min -> max -> min (current).
* We tried to go into that direction, but aborted because too much energy
  was used.

If an extreme that is found is "immediately" next to the previous one
or not depends on whether or not a "reset" found place in between.
An algorithm is used to make a it very unlikely that we jump over an
existing extreme (if that happens, then it is lost; we don't know it
exists). Normally we alternating find minima and maxima that are
all adjacent. However, if we find a sample that tells us that we
(might have) jumped too far, but it is lower than the lowest point
found so far, then a "reset" happens, and we continue at that point
as if we just started (with the exception that samples already taken
are remembered).

The Algorithm has several modes it can be in:
A) Looking for an extreme with just one sample (Initial sample).
B) Looking for an extreme with two samples.
C) Looking for a minimum with just one sample (Just being reset).
D) Looking for a minimum left of two given samples.
E) Looking for a minimum between two given samples.
F) Looking for a minimum right of two given samples.
G) Looking for a maximum left of two given samples.
H) Looking for a maximum between two given samples.
I) Looking for a maximum right of two given samples.

The Algorithm has a member hdirection_ that contains the
direction that we want to go in after finding the next
local extreme: this value is set at the moment an extreme
is found, because then it is decided if want to go look
for the next extreme left or right of that one. This direction
is reset to "undecided" whenever a jump takes place: either
one where we jumped over extremes (but to a lower point)
or when we jumped back to the best minimum so far.

Below we use the fact that if hdirection_ means 'left' then
its value is -1, and if it means 'right' its value is 1.

The Algorithm has a member small_step_ that is initialized
with the value zero, meaning it is unknown. If possible it
is set to the absolute value of the "scale" of the current
parabolic approximation.                                        //FIXME: parabolic? or another approximation as well?
As long as small_step_ is unknown, we fall back to a constant
given by learning_rate_.

The following sub-algorithms exist:
A/C)
 * small_step_ is unknown (zero), in this case learning_rate_ is used:
   - derivative is zero (`almost_zero`).
     > hdirection_ is undecided.
         In this case step is set to learning_rate_.
     > hdirection_ is known.                                                    } - never happens: small_step_ is set once hdirection_ is known.
         In this case step is set to hdirection_ * learning_rate_.              }
   - derivative is not zero.
     > hdirection_ is undecided.
         In this case step is set to learning_rate_ * -dLdW (the normal gradient descent).
     > hdirection_ is known.                                                    } - idem
         In this case step is set to hdirection_ * abs(learning_rate_ * dLdW).  }
 * small_step_ is known.
     In this case step is set to hdirection_ * small_step_.

  When having just one sample (A and C) the step is never so small that it will
  be considered `negligible`; if that is the case then it is increased until it
  no longer is `negligible`. Here `negligible` is the same algorithm that is used
  to decide if a new `w` should replace the previous sample instead of being added
  as a new sample. As a result, we only have a single sample once; the newly
  returned w then will always be considered to be a new, second sample. Therefore
  the Algorithm will always leave mode A and C after one step.


